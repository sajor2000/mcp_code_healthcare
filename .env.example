# Healthcare Research MCP Server Configuration
# Copy this file to .env and fill in your actual API keys

# Database Configuration
ONTOLOGY_DB_PATH=./data/databases/ontology.db
RESEARCH_DB_PATH=./data/databases/research.db

# API Keys for External Search and Data Collection
# Brave Search API (https://brave.com/search/api/)
BRAVE_API_KEY=your_brave_api_key_here

# Perplexity API (https://www.perplexity.ai/api)
PERPLEXITY_API_KEY=your_perplexity_api_key_here

# Firecrawl API (for web scraping)
FIRECRAWL_API_KEY=your_firecrawl_api_key_here

# ==============================================================================
# LLM Configuration - Multiple Provider Support
# ==============================================================================
# The system will auto-detect available providers based on API keys
# Or you can explicitly set a provider:
LLM_PROVIDER=anthropic  # Options: anthropic, openai, google, mistral, cohere, ollama, lmstudio, custom

# Cloud Provider API Keys (set the ones you want to use)
# ------------------------------------------------------------------------------
# Anthropic Claude
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# OpenAI GPT
OPENAI_API_KEY=your_openai_api_key_here

# Google Gemini/PaLM
GOOGLE_API_KEY=your_google_api_key_here

# Mistral AI
MISTRAL_API_KEY=your_mistral_api_key_here

# Cohere
COHERE_API_KEY=your_cohere_api_key_here

# Local Model Configuration
# ------------------------------------------------------------------------------
# Ollama (runs models locally)
OLLAMA_BASE_URL=http://localhost:11434  # Default Ollama endpoint
OLLAMA_MODEL=llama3:70b  # Or meditron:70b for medical-specialized

# LM Studio (local models with OpenAI-compatible API)
LMSTUDIO_BASE_URL=http://localhost:1234  # Default LM Studio endpoint
LMSTUDIO_MODEL=TheBloke/meditron-70B-GGUF  # Example medical model

# Custom Provider (any OpenAI-compatible endpoint)
CUSTOM_BASE_URL=https://your-api-endpoint.com
CUSTOM_API_KEY=your_custom_api_key
CUSTOM_MODEL=your-model-name

# Model Selection
# ------------------------------------------------------------------------------
# Specific model override (optional - defaults are optimized for each provider)
# LLM_MODEL=claude-3-opus-20240229  # Anthropic
# LLM_MODEL=gpt-4-turbo-preview     # OpenAI
# LLM_MODEL=gemini-pro              # Google
# LLM_MODEL=mistral-large-latest    # Mistral
# LLM_MODEL=command-r-plus          # Cohere
# LLM_MODEL=llama3:70b              # Ollama
# LLM_MODEL=meditron:70b            # Medical-specialized Ollama model

# Advanced LLM Settings
# ------------------------------------------------------------------------------
LLM_TEMPERATURE=0.1  # Lower = more deterministic (0.0-1.0)
LLM_MAX_TOKENS=4000  # Maximum response length
LLM_TIMEOUT=60000    # Request timeout in milliseconds

# Medical Model Preference
PREFER_MEDICAL_MODELS=false  # Set to true to use medical-specialized models when available

# Fallback Providers (comma-separated, used if primary fails)
LLM_FALLBACK_PROVIDERS=ollama,lmstudio  # Will try local models if cloud fails

# Server Configuration
PORT=3000
LOG_LEVEL=info  # Options: error, warn, info, debug

# Security & Compliance
HIPAA_COMPLIANT=true
AUDIT_LEVEL=detailed  # Options: basic, detailed, verbose
ENABLE_PHI_PROTECTION=true
MIN_CELL_SIZE=10

# Performance
ENABLE_CACHE=true
CACHE_TTL=3600  # Cache time-to-live in seconds
MAX_CONCURRENT_ANALYSES=5

# Data Model Settings
DEFAULT_DATA_MODEL=OMOP  # Options: OMOP, CLIF

# Optional: External Database Connection (for larger deployments)
# DB_TYPE=postgres
# DB_HOST=localhost
# DB_PORT=5432
# DB_NAME=healthcare_research
# DB_USER=healthcare_user
# DB_PASSWORD=secure_password

# Optional: Redis Cache (for distributed deployments)
# REDIS_URL=redis://localhost:6379

# Development Settings
NODE_ENV=development  # Options: development, production, test
DISABLE_CACHE=false  # Set to true for development